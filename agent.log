Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/src/player/agentic_player/AgenticPlayer.py", line 82, in decide_action
    res = await Runner.run(
          ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 345, in run
    return await runner.run(
           ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 597, in run
    raise MaxTurnsExceeded(f"Max turns ({max_turns}) exceeded")
agents.exceptions.MaxTurnsExceeded: Max turns (1) exceeded
Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/src/player/agentic_player/AgenticPlayer.py", line 82, in decide_action
    res = await Runner.run(
          ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 345, in run
    return await runner.run(
           ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 597, in run
    raise MaxTurnsExceeded(f"Max turns ({max_turns}) exceeded")
agents.exceptions.MaxTurnsExceeded: Max turns (1) exceeded
Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/src/player/agentic_player/AgenticPlayer.py", line 82, in decide_action
    res = await Runner.run(
          ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 345, in run
    return await runner.run(
           ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 597, in run
    raise MaxTurnsExceeded(f"Max turns ({max_turns}) exceeded")
agents.exceptions.MaxTurnsExceeded: Max turns (2) exceeded
Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 157, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py", line 464, in post
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py", line 420, in post
    response.raise_for_status()
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/main.py", line 609, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 289, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 182, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 3617, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openrouter.common_utils.OpenRouterException: {"error":{"message":"Provider returned error","code":400,"metadata":{"raw":"Gemini models require OpenRouter reasoning details to be preserved in each request. Please refer to our docs: https://openrouter.ai/docs/guides/best-practices/reasoning-tokens#preserving-reasoning-blocks. Upstream error: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Unable to submit request because function call `default_api:mark_julio_for_hunting` in the 2. content block is missing a `thought_signature`. Learn more: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n","provider_name":"Google"}},"user_id":"user_32gNDclXUqlvXwRPp3UNvrV4zbH"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/src/player/agentic_player/AgenticPlayer.py", line 82, in decide_action
    res = await Runner.run(
          ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 345, in run
    return await runner.run(
           ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 626, in run
    turn_result = await self._run_single_turn(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 1470, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 1725, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/extensions/models/litellm_model.py", line 100, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/extensions/models/litellm_model.py", line 380, in _fetch_response
    ret = await litellm.acompletion(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1666, in wrapper_async
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1512, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/main.py", line 628, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2202, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: litellm.BadRequestError: OpenrouterException - {"error":{"message":"Provider returned error","code":400,"metadata":{"raw":"Gemini models require OpenRouter reasoning details to be preserved in each request. Please refer to our docs: https://openrouter.ai/docs/guides/best-practices/reasoning-tokens#preserving-reasoning-blocks. Upstream error: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Unable to submit request because function call `default_api:mark_julio_for_hunting` in the 2. content block is missing a `thought_signature`. Learn more: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n","provider_name":"Google"}},"user_id":"user_32gNDclXUqlvXwRPp3UNvrV4zbH"}
Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 157, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py", line 464, in post
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py", line 420, in post
    response.raise_for_status()
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/main.py", line 609, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 289, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 182, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 3617, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openrouter.common_utils.OpenRouterException: {"error":{"message":"Provider returned error","code":400,"metadata":{"raw":"Gemini models require OpenRouter reasoning details to be preserved in each request. Please refer to our docs: https://openrouter.ai/docs/guides/best-practices/reasoning-tokens#preserving-reasoning-blocks. Upstream error: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function call is missing a thought_signature in functionCall parts. This is required for tools to work correctly, and missing thought_signature may lead to degraded model performance. Additional data, function call `default_api:mark_julio_for_hunting` , position 2. Please refer to https://ai.google.dev/gemini-api/docs/thought-signatures for more details.\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n","provider_name":"Google AI Studio"}},"user_id":"user_32gNDclXUqlvXwRPp3UNvrV4zbH"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/src/player/agentic_player/AgenticPlayer.py", line 82, in decide_action
    res = await Runner.run(
          ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 345, in run
    return await runner.run(
           ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 626, in run
    turn_result = await self._run_single_turn(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 1470, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 1725, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/extensions/models/litellm_model.py", line 100, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/extensions/models/litellm_model.py", line 380, in _fetch_response
    ret = await litellm.acompletion(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1666, in wrapper_async
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1512, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/main.py", line 628, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2202, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: litellm.BadRequestError: OpenrouterException - {"error":{"message":"Provider returned error","code":400,"metadata":{"raw":"Gemini models require OpenRouter reasoning details to be preserved in each request. Please refer to our docs: https://openrouter.ai/docs/guides/best-practices/reasoning-tokens#preserving-reasoning-blocks. Upstream error: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function call is missing a thought_signature in functionCall parts. This is required for tools to work correctly, and missing thought_signature may lead to degraded model performance. Additional data, function call `default_api:mark_julio_for_hunting` , position 2. Please refer to https://ai.google.dev/gemini-api/docs/thought-signatures for more details.\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n","provider_name":"Google AI Studio"}},"user_id":"user_32gNDclXUqlvXwRPp3UNvrV4zbH"}
Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 157, in _make_common_async_call
    response = await async_httpx_client.post(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py", line 464, in post
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py", line 420, in post
    response.raise_for_status()
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/main.py", line 609, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 289, in async_completion
    response = await self._make_common_async_call(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 182, in _make_common_async_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py", line 3617, in _handle_error
    raise provider_config.get_error_class(
litellm.llms.openrouter.common_utils.OpenRouterException: {"error":{"message":"Provider returned error","code":400,"metadata":{"raw":"Gemini models require OpenRouter reasoning details to be preserved in each request. Please refer to our docs: https://openrouter.ai/docs/guides/best-practices/reasoning-tokens#preserving-reasoning-blocks. Upstream error: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function call is missing a thought_signature in functionCall parts. This is required for tools to work correctly, and missing thought_signature may lead to degraded model performance. Additional data, function call `default_api:mark_julio_for_hunting` , position 2. Please refer to https://ai.google.dev/gemini-api/docs/thought-signatures for more details.\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n","provider_name":"Google AI Studio"}},"user_id":"user_32gNDclXUqlvXwRPp3UNvrV4zbH"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/adrian/Code/agentic_werewolf/src/player/agentic_player/AgenticPlayer.py", line 82, in decide_action
    res = await Runner.run(
          ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 345, in run
    return await runner.run(
           ^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 626, in run
    turn_result = await self._run_single_turn(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 1470, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/run.py", line 1725, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/extensions/models/litellm_model.py", line 100, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/agents/extensions/models/litellm_model.py", line 380, in _fetch_response
    ret = await litellm.acompletion(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1666, in wrapper_async
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1512, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/main.py", line 628, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "/home/adrian/Code/agentic_werewolf/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2202, in exception_type
    raise BadRequestError(
litellm.exceptions.BadRequestError: litellm.BadRequestError: OpenrouterException - {"error":{"message":"Provider returned error","code":400,"metadata":{"raw":"Gemini models require OpenRouter reasoning details to be preserved in each request. Please refer to our docs: https://openrouter.ai/docs/guides/best-practices/reasoning-tokens#preserving-reasoning-blocks. Upstream error: {\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Function call is missing a thought_signature in functionCall parts. This is required for tools to work correctly, and missing thought_signature may lead to degraded model performance. Additional data, function call `default_api:mark_julio_for_hunting` , position 2. Please refer to https://ai.google.dev/gemini-api/docs/thought-signatures for more details.\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}\n","provider_name":"Google AI Studio"}},"user_id":"user_32gNDclXUqlvXwRPp3UNvrV4zbH"}
